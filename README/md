# Як запустити ETL вручну
Спочатку потрібно зайти у репозиторій на GitHub, обрати вгорі вкладку Actions.
Потім зліва обрати Simple ETL та натиснути Run workflow. Зачекати 1-2 хвилини,
та після завершення процесу запуску з'явиться вкладка Artifacts з файлом
etl-output.zip (там будуть графіки, EXCEL таблиця з топ-клієнтами та CSV файл з результатами).

# Як працює СІ/CD
CI (continuous integration) - щоразу, коли я додаю або змінюю файли,
GitHub автоматично запускає ETL-скрипт і перевіряє, чи все працює.

CD (continuous delivery) - після успішного запуску результати (файли, графіки)
зберігаються у вигляді артефактів, які я можу скачати.

# Як би я адаптувала рішення під 100+ млн рядків
## Які технології заміню/додам
1. Замість CSV формату я б використовувала збереження у хмарі (наприклад Google Cloud Storage)
2. Замінила б pandas на інструменти, які можуть працювати з великими наборами даних
   (наприклад Dask/Polars)
3. SQLite замінила б на щось більш потужне - наприклад PostgreSQL
   
## Яку архітектуру ETL я б запропонувала
1. Extract - дані завантажуються з хмарного сховища (наприклад S3)
2. Transform - обробка даниз через Dask
3. Load - збереження у хмарну базу даних (наприклад PostgreSQL)
   
## Які метрики моніторингу ETL я б додала
1. Час виконання кожного етапу (Extract, Transform, Load)
2. Кількість оброблених рядків
3. Чи були пропущені або зіпсовані рядки (наприклад, з помилками у датах)
4. Помилки або збої
   
## Де будуть зберігатись вхідні і вихідні дані
Я б зберігала вхідні дані у хмарному сховищі, наприклад Google Cloud Storage або Amazon S3.
Туди можна завантажувати великі файли і потім зручно їх обробляти.

Вихідні дані я б зберігала у базі даних — наприклад, PostgreSQL або BigQuery.
Там можна робити аналітику, будувати графіки або передавати результати далі.

Якщо було б потрібно ще зберегти звіти чи графіки, то їх зберігала б у хмару
або в окрему папку — наприклад, щоб потім подивитися або надіслати колегам.
